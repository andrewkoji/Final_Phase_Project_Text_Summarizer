{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, Concatenate, TimeDistributed, Bidirectional, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
       "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
       "\n",
       "                                             article  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Bishop John Folda, of North Dakota, is taking ...  \n",
       "1  Criminal complaint: Cop used his role to help ...  \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
       "3  Nina dos Santos says Europe must be ready to a...  \n",
       "4  Fleetwood top of League One after 2-0 win at S...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(['id'], axis=1)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "- Preprocessing of text data varies from one NLP task to another\n",
    "- Typically in text classification tasks, you remove the stopwords from the sentence and also stem or lemmatize the words to their corresponding root words to reduce the vocabulary size. However, in language modelling we will refrain from doing so as these words can help our model undertand the underlying semantics of the language for it to produce texts of its own\n",
    "- Therefore we will limit our text preprocessing to the following steps:\n",
    "1. Remove any extra lines or spaces from the text\n",
    "2. Remove any characters other than alphabets, numbers or period\n",
    "3. Tokenize the text\n",
    "4. Add a start and end token for the model to identify when the sentence starts and ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    tmp = []\n",
    "    for word in text:\n",
    "        if word in contractions:\n",
    "            tmp.append(contractions[word])\n",
    "        else:\n",
    "            tmp.append(word)\n",
    "    text = ' '.join(tmp)\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = ' '.join(text)\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alevi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Summaries Complete\n",
      "Cleaning Texts Complete\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "clean_summaries = []\n",
    "for summary in train_data.highlights:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print('Cleaning Summaries Complete')\n",
    "    \n",
    "clean_texts = []\n",
    "for text in train_data.article:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print('Cleaning Texts Complete')\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame()\n",
    "clean_df['text'] = clean_texts[:110000]\n",
    "clean_df['summary'] = clean_summaries[:110000]\n",
    "clean_df['summary'].replace('', np.nan, inplace=True)\n",
    "clean_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "clean_df['summary'] = clean_df['summary'].apply(lambda x: '<sostok>' + ' ' + x + ' ' + '<eostok>')\n",
    "del clean_texts\n",
    "del clean_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text And Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(clean_df['text'], clean_df['summary'], test_size=0.1, random_state=0)\n",
    "del clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_tokenizer = Tokenizer()\n",
    "t_tokenizer.fit_on_texts(list(train_x))\n",
    "\n",
    "thresh = 4\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in t_tokenizer.word_counts.items():\n",
    "    total_count += 1\n",
    "    total_frequency += value\n",
    "    if value < thresh:\n",
    "        count += 1\n",
    "        frequency += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  60.00719413677853\n",
      "Total Coverage of rare words:  0.7458147584629847\n",
      "Text Vocab:  133418\n"
     ]
    }
   ],
   "source": [
    "print('% of rare words in vocabulary: ', (count/total_count)*100.0)\n",
    "print('Total Coverage of rare words: ', (frequency/total_frequency)*100.0)\n",
    "t_max_features = total_count - count\n",
    "print('Text Vocab: ', t_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_tokenizer = Tokenizer()\n",
    "s_tokenizer.fit_on_texts(list(train_y))\n",
    "\n",
    "thresh = 6\n",
    "count = 0\n",
    "total_count = 0\n",
    "frequency = 0\n",
    "total_frequency = 0\n",
    "\n",
    "for key, value in s_tokenizer.word_counts.items():\n",
    "    total_count += 1\n",
    "    total_frequency += value\n",
    "    if value < thresh:\n",
    "        count += 1\n",
    "        frequency += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  70.11489659306625\n",
      "Total Coverage of rare words:  2.4837378859339996\n",
      "Summary Vocab:  29912\n"
     ]
    }
   ],
   "source": [
    "print('% of rare words in vocabulary: ', (count/total_count)*100.0)\n",
    "print('Total Coverage of rare words: ', (frequency/total_frequency)*100.0)\n",
    "s_max_features = total_count-count\n",
    "print('Summary Vocab: ', s_max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_text = 750\n",
    "maxlen_summ = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [19:31<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "val_x = test_x\n",
    "t_tokenizer = Tokenizer(num_words=t_max_features)\n",
    "t_tokenizer.fit_on_texts(list(train_x))\n",
    "train_x = t_tokenizer.texts_to_sequences(train_x)\n",
    "val_x = t_tokenizer.texts_to_sequences(val_x)\n",
    "\n",
    "train_x = pad_sequences(train_x, maxlen=maxlen_text, padding='post')\n",
    "val_x = pad_sequences(val_x, maxlen=maxlen_text, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y = test_y\n",
    "s_tokenizer = Tokenizer(num_words=s_max_features)\n",
    "s_tokenizer.fit_on_texts(list(train_y))\n",
    "train_y = s_tokenizer.texts_to_sequences(train_y)\n",
    "val_y = s_tokenizer.texts_to_sequences(val_y)\n",
    "\n",
    "train_y = pad_sequences(train_y, maxlen=maxlen_summ, padding='post')\n",
    "val_y = pad_sequences(val_y, maxlen=maxlen_summ, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sequence (99000, 750)\n",
      "Target Values Shape (99000, 100)\n",
      "Test Sequence (11000, 750)\n",
      "Target Test Shape (11000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Sequence\", train_x.shape)\n",
    "print('Target Values Shape', train_y.shape)\n",
    "print('Test Sequence', val_x.shape)\n",
    "print('Target Test Shape', val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_index = {}\n",
    "embed_dim = 100\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embed = np.zeros((t_max_features, embed_dim))\n",
    "for word, i in t_tokenizer.word_index.items():\n",
    "    vec = embeding_index.get(word)\n",
    "    if i < t_max_features and vec is not None:\n",
    "        t_embed[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_embed = np.zeros((s_max_features, embed_dim))\n",
    "for word, i in s_tokenizer.word_index.items():\n",
    "    vec = embeding_index.get(word)\n",
    "    if i < s_max_features and vec is not None:\n",
    "        s_embed[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 750)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 750, 100)     13341800    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  [(None, 256),       234496      ['embedding_2[0][0]']            \n",
      " )                               (None, 128),                                                     \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 100)    2991200     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " enc_h (Concatenate)            (None, 256)          0           ['bidirectional_1[0][1]',        \n",
      "                                                                  'bidirectional_1[0][3]']        \n",
      "                                                                                                  \n",
      " enc_c (Concatenate)            (None, 256)          0           ['bidirectional_1[0][2]',        \n",
      "                                                                  'bidirectional_1[0][4]']        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 256),  365568      ['embedding_3[0][0]',            \n",
      "                                 (None, 256),                     'enc_h[0][0]',                  \n",
      "                                 (None, 256)]                     'enc_c[0][0]']                  \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 29912)  7687384    ['lstm_3[0][0]']                 \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,620,448\n",
      "Trainable params: 8,287,448\n",
      "Non-trainable params: 16,333,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "# Encoder\n",
    "enc_input = Input(shape=(maxlen_text, ))\n",
    "enc_embed = Embedding(t_max_features, embed_dim, input_length=maxlen_text, weights=[t_embed], trainable=False)(enc_input)\n",
    "# h_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# h_out, _, _ = h_lstm(enc_embed)\n",
    "enc_lstm = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "enc_output, enc_fh, enc_fc, enc_bh, enc_bc = enc_lstm(enc_embed)\n",
    "enc_h = Concatenate(axis=-1, name='enc_h')([enc_fh, enc_bh])\n",
    "enc_c = Concatenate(axis=-1, name='enc_c')([enc_fc, enc_bc])\n",
    "#Decoder\n",
    "dec_input = Input(shape=(None, ))\n",
    "dec_embed = Embedding(s_max_features, embed_dim, weights=[s_embed], trainable=False)(dec_input)\n",
    "dec_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\n",
    "dec_outputs, _, _ = dec_lstm(dec_embed, initial_state=[enc_h, enc_c])\n",
    "\n",
    "dec_dense = TimeDistributed(Dense(s_max_features, activation='softmax'))\n",
    "dec_output = dec_dense(dec_outputs)\n",
    "\n",
    "model = Model([enc_input, dec_input], dec_output)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  3/774 [..............................] - ETA: 16:11:37 - loss: 9.9965 "
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "model.fit([train_x, train_y[:, :-1]], train_y.reshape(train_y.shape[0], train_y.shape[1], 1)[:, 1:], epochs=20, callbacks=[early_stop], batch_size=128, verbose=1, validation_data=([val_x, val_y[:, :-1]], val_y.reshape(val_y.shape[0], val_y.shape[1], 1)[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
